 
\documentclass[12pt]{article}
 %author David Dobor
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 \usepackage{graphicx}
 \usepackage{multirow}
\usepackage[scaled]{helvet}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[T1]{fontenc}
\usepackage{palatino}
\usepackage{enumerate}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}


\newcommand{\blditA}{\textbf{\textit{A}}}
\newcommand{\blditB}{\textbf{\textit{B}}}
\newcommand{\blditC}{\textbf{\textit{C}}}
\newcommand{\blditP}{\textbf{\textit{P}}}
\newcommand{\blditQ}{\textbf{\textit{Q}}}
\newcommand{\bldI}{\textbf{I}}
\newcommand{\blditX}{\textbf{\textit{X}}}

 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{answer}[2][Answer]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
 \renewcommand{\arraystretch}{1.3}

 
\title{My Notes About $\Gamma$}%replace X with the appropriate number
\author{By David Dobor} %if necessary, replace with your course title
\date{}

\maketitle
 A random variable $X$ is said to have a gamma distribution with parameters $(\alpha, \beta)$, denoted by  $\Gamma(\alpha, \beta)$, if its  pdf is given by:
$$
		f(x) = \frac{1}{\Gamma({\alpha})} \cdot \frac{1}{\beta} \cdot  \left( \frac{x}{\beta}\right)^{\alpha - 1} \mathrm{e}^{-\frac{x}{\beta}} \;\;\;\;\;\; \text{where} \;\;  x \ge 0 \;\; \alpha, \beta > 0
$$

Here $\Gamma(\alpha)$ is the gamma function computed as $\int_0^{\infty} \mathrm{e}^{-y} y^{\alpha - 1} dy$. (This reduces to the factorial function for integer $\alpha$: $\Gamma(\alpha) = \alpha!$)


\bigskip

\textbf{\color{TealBlue}\emph{The Moment Generating Function:} } 
\begin{align*}
M_X(t) &= \mathrm{E} \: \mathrm{e}^{tX} \\ 
&=  \frac{1}{\Gamma(\alpha) \cdot \beta^{\alpha}} \int_0^{\infty} \mathrm{e}^{tx} x^{\alpha - 1} \mathrm{e}^{-x/\beta} dx\\
&=  \frac{1}{\Gamma(\alpha) \cdot \beta^{\alpha}} \int_0^{\infty}  x^{\alpha - 1} \mathrm{e}^{ - x/ \frac{\beta}{1 - \beta t }} dx \\
\end{align*}

To see what this integrates to, use this common trick: leave inside the integral a function that's easily recognizeable to be a \texttt{pdf}, which
would therefore integrate to 1, and pull remaining constants out of the integral sign:
\begin{align*}
M_X(t) &= \frac{1}{\Gamma(\alpha)} \frac{1}{\beta^{\alpha}}\left( \frac{\beta}{1 - \beta t} \right)^\alpha \int_0^{\infty} \frac{1} { \left( \frac{\beta}{1 - \beta t} \right)^\alpha} \mathrm{e}^{tx} x^{\alpha - 1} \mathrm{e}^{-x/\beta} dx\\
 &= \frac{1}{\beta^{\alpha}}\left( \frac{\beta}{1 - \beta t} \right)^\alpha \int_0^{\infty} \frac{1}{\Gamma(\alpha)} \: \frac{1} { \left( \frac{\beta}{1 - \beta t} \right)^\alpha} \mathrm{e}^{tx} x^{\alpha - 1} \mathrm{e}^{-x/\beta} dx\\
&= \left( \frac{1} { 1 - \beta t } \right)^{\alpha}
\end{align*}

Where the last equality follows because the thing inside the integral is a \texttt{Gamma} \texttt{pdf}. 
 
 \textbf{\color{TealBlue}\emph{First moment, the expectation, by direct integration:} } 
\begin{align*}
 \mathrm{E} \: X &= \int_0^{\infty}  \frac{1}{\Gamma({\alpha})} \cdot \frac{1}{\beta} \cdot x  \left( \frac{x}{\beta}\right)^{\alpha - 1} \mathrm{e}^{-\frac{x}{\beta}} dx \\
&=   \frac{1}{\Gamma({\alpha})} \int_0^{\infty} \left( \frac{x}{\beta}\right)^{\alpha} \mathrm{e}^{-\frac{x}{\beta}} dx \\
&=   \frac{\beta}{\Gamma({\alpha})} \int_0^{\infty} \frac{1}{\beta} \left( \frac{x}{\beta}\right)^{\alpha} \mathrm{e}^{-\frac{x}{\beta}} dx \\
&=   \frac{ \Gamma({\alpha} + 1) \cdot \beta }{\Gamma({\alpha})} \int_0^{\infty} \frac{1}{\Gamma(\alpha + 1)} \frac{1}{\beta} \left( \frac{x}{\beta}\right)^{\alpha} \mathrm{e}^{-\frac{x}{\beta}} dx \\   
&=  \frac{ \Gamma({\alpha} + 1) \cdot \beta }{\Gamma({\alpha})}  = \alpha\beta
\end{align*}
where the last integrand is $1$ because it itself is a gamma pdf with parameters $(\alpha + 1, \beta)$. 


\bigskip

 \textbf{\color{TealBlue}\emph{Second moment, by direct integration:} } 
\begin{align*}
 \mathrm{E} \: X^2 &= \int_0^{\infty}  \frac{1}{\Gamma({\alpha})} \cdot \frac{1}{\beta} \cdot x^2  \left( \frac{x}{\beta}\right)^{\alpha - 1} \mathrm{e}^{-\frac{x}{\beta}} dx \\
&=   \frac{\Gamma(\alpha + 2) \cdot \beta^2}{\Gamma({\alpha})} \int_0^{\infty} \frac{1}{\Gamma(\alpha + 2)} \frac{1}{\beta} \left( \frac{x}{\beta}\right)^{\alpha +1 } \mathrm{e}^{-\frac{x}{\beta}} dx \\
&=  \frac{ \Gamma({\alpha} + 2) \cdot \beta^2 }{\Gamma({\alpha})}  = \alpha (\alpha + 1) \beta^2
\end{align*}

\textbf{\color{TealBlue}\emph{The Variance is then:} } 
$$
Var(X) = \alpha (\alpha + 1) \beta^2 - (\alpha \beta)^2 = \alpha \beta^2
$$\\

\textbf{\color{TealBlue}\emph{Moments from the Moment Generating Function:} } \\
\\
Differentiate the Moment Generating Function (mgf) appropriate number of times and evaluate the derivative at $t = 0$ to get the moments: differentiate once to get the expectation, twice to get the second moment, etc.. E.g.:
\begin{align*}
(M_X(t))^{'} = \frac{\alpha \beta} { (1 - \beta t)^{\alpha + 1}}  
\end{align*}
\begin{align*}
(M_X(0))^{'} =  \frac{\alpha \beta} { (1 - \beta t)^{\alpha + 1}}  \Big | _{t=0} = \alpha \beta
\end{align*}

\begin{align*}
(M_X(0))^{''} =  \frac{\alpha (\alpha + 1) \beta^2} { (1 - \beta t)^{\alpha + 2}}  \Big | _{t=0} = \alpha (\alpha + 1)\beta^2
\end{align*}

\newpage

%%%%%%%%%%%%%%% LogOfGamma %%%%%%%%%%%%%%%%%%%%%%
\begin{center}
\subsection*{\color{TealBlue}\emph{Distribution of Log of Gamma:} }
\end{center}

We are interested in the distribution of the natural logarithm of Gamma. \\
Let $Y = \log (X)$ where  $X\sim\Gamma(\alpha,\beta)$.  We want the distribution of $Y$. \\

We'll use the following well known theorem about computing the density of a transform, $g$, of some random variable $X$. Loosely put, if $g(X) = Y$ and $g$ has an \emph{inverse}, call it $h(Y)$, then the \texttt{pdf} of $Y$ is:
$$
f_Y(y) = f_X(h(y)) |h'(y)|
$$

So here, denoting the inverse of $Y = \log (X)$ by $h(Y)$, we have $ X=h(Y)=\mathrm{e}^Y$ and $h'(y) = \mathrm{e}^{y}$. Therefore:
\begin{align*}
 f_Y(y) &= f_X(h(y)) |h'(y)| \\
 &= \frac{1}{\beta^\alpha \Gamma(\alpha)} \left(\mathrm{e}^{\alpha (y - 1)} - \mathrm{e}^{-\mathrm{e}^{y/\beta}}\right)\,|\mathrm{e}^{y}| \,\,\,1_{(-\infty,\infty)}(y) \\
  &= \frac{1}{\beta^\alpha \Gamma(\alpha)} \left(\mathrm{e}^{\alpha y} - \mathrm{e}^{-\mathrm{e}^{y/\beta}}\right)\,\mathrm{e}^{-y}\mathrm{e}^{y} \,\,\,1_{(-\infty,\infty)}(y) \\
 &= \frac{1}{\beta^\alpha \Gamma(\alpha)} \;\; \mathrm{e}^{\left(\alpha y - e^{y/\beta}\right)}\,1_{(-\infty,\infty)}(y) \\
\end{align*}

This expression  is a valid \texttt{pdf} -- it integrates to 1 - and is said to have a  \texttt{Log-Of-Gamma pdf} with parameters $(\alpha, \beta)$.



\bigskip 
\bigskip 
%%%%%%%%%%%%%%% Moments of Log-Of-Gamma %%%%%%%%%%%%%%%%%%%%%%
{\color{TealBlue}\emph{Moments of Log Of Gamma}}\\

We now find the Moment Generating Function (\texttt{MGF}) of \texttt{Log-Of-Gamma}$(\alpha, \beta)$. By definition:
$$
M_Y(t) = \int_{-\infty}^{+\infty} \mathrm{e}^{ty} \frac{1}{\beta^\alpha \Gamma(\alpha)} \;\; \mathrm{e}^{\left(\alpha y - e^{y\beta}\right)} dy
$$
To integrate this we use an old trick: we try to leave inside the integral sign the part of the function that integrates to 1 (this is often an easily recognizeable \texttt{pdf}); we then pull the constants that get in the way of making this integral equal to 1 out of the integral sign. Luckily, this can be done here:
\begin{align*}
M_Y(t) &= \frac{1}{\beta^\alpha \; \Gamma(\alpha)} \int_{-\infty}^{+\infty} \mathrm{e}^{ty} \;\; \mathrm{e}^{\left(\alpha y - e^{y/\beta}\right)} dy\\
&= \frac{1}{\beta^\alpha \; \Gamma(\alpha)} \int_{-\infty}^{+\infty} \mathrm{e}^{\left(ty +\alpha y - e^{y/\beta}\right)} dy\\
&= \frac{1}{\beta^\alpha \; \Gamma(\alpha)} \cdot \frac{\beta^{\alpha + t} \; \Gamma(\alpha + t)}{\beta^{\alpha + t} \; \Gamma(\alpha + t)} \cdot \int_{-\infty}^{+\infty} \mathrm{e}^{\left(y(\alpha + t) - e^{y/\beta}\right)} dy\\
&= \frac{ \beta^{\alpha + t} \; \Gamma(\alpha + t) }{\beta^\alpha \; \Gamma(\alpha)}  \cdot \int_{-\infty}^{+\infty} \frac{1}{\beta^{t+\alpha} \; \Gamma(\alpha + t)} \mathrm{e}^{\left(y(\alpha + t) - e^{y/\beta}\right)} dy\\
&= \frac{\beta^t \; \Gamma(\alpha + t)}{\Gamma(\alpha)} 
\end{align*}

Where the last equality follows because the thing inside the integral is the \texttt{Log-Of-Gamma pdf} derived above and integrates to 1.
\bigskip
$$\text{\emph{In Short:}}$$
$$
\boxed{M_Y(t) = \frac{\beta^t \; \Gamma(\alpha + t)}{\Gamma(\alpha)}   \; \; \; \; \text{when $Y$ is \texttt{Log-Of-Gamma($\alpha, \beta$)}} }
$$

\bigskip 
\bigskip 
%%%%%%%%%%%%%%% Cumulant Generating Function %%%%%%%%%%%%%%%%%%%%%%

{\color{TealBlue}\emph{Cumulant Generating Function} }\\

The Cumulant Generating Function (\texttt{CGF}) of a random variable $Y$ is defined as:
$$
S_Y(t) = \log (M_Y(t))
$$

It can be verified that:


\begin{align*}
\frac{d}{dt} S_Y(t) \Big|_{t=0} = \mathrm{E}\, Y \;\;\; \text{ and} \;\;\; \frac{d^2}{dt^2} S_Y(t) \Big|_{t=0} = Var(Y) 
\end{align*}

\bigskip 
\bigskip 
%%%%%%%%%%%%%%% The Moments of Log Of Gamma %%%%%%%%%%%%%%%%%%%%%%

{\color{TealBlue}\emph{Expectation and Variance of Log Of Gamma} }\\

To find the expectation of \texttt{Log-Of-Gamma}, we differentiate $S_Y(t) = \log (M_Y(t))$ and evaluate the result at $t = 0$.

\begin{align*}
(S_Y(t))^{'} &= (\log (M_Y(t)))^{'} = \left(\log \left(\frac{\beta^t \; \Gamma(\alpha + t)}{\Gamma(\alpha)} \right)\right)^{'} \\
&= \frac{1}{\frac{\beta^t \; \Gamma(\alpha + t)} {\Gamma(\alpha)}} \cdot \frac{1}{\Gamma(\alpha)} \cdot \left(  \beta^t \: \Gamma(\alpha + t)  \right)^{'}\\
&= \frac{\Gamma(\alpha)}{\beta^t \; \Gamma(\alpha + t)}  \cdot \frac{1}{\Gamma(\alpha)} \cdot \left(  \beta^t \log(\beta) \; \Gamma(\alpha + t) + \beta^t \; \Gamma^{'} (\alpha +t) \right)\\
&= \log(\beta) + \frac{\Gamma^{'} (\alpha +t) }{\Gamma(\alpha + t)}\\
\end{align*}


The last term is the log-derivative of the Gamma function evaluated at $\alpha + t$. This log-derivative is called the \texttt{digamma} function and is denoted by $\psi(\cdot)$. Thus:
$$
\boxed{(S_Y(t))^{'} =  \log(\beta) +\psi(\alpha +t)}
$$

We \emph{conclude} that if $Y$ is \texttt{Log-Of-Gamma}$(\alpha, \beta)$ distributed, then its expectation is given by:
$$
 \boxed{\mathrm{E}\, Y =  \log(\beta) +\psi(\alpha)}
$$
To find the variance of $Y$, we we look at the second derivative, $(S_Y(t))^{''}$, and evaluate it at $t=0$.
\begin{align*}
(S_Y(t))^{''} &= (\log(\beta) +\psi(\alpha +t))^{'}\\
&= \psi^{'}(\alpha +t)
\end{align*}

The derivative of \texttt{digamma} is the so-called \texttt{trigamma} function:
%\begin{align*}
$$
\boxed{Var(Y) = \psi^{''}(\alpha) = \text{\texttt{trigamma($\alpha$)}}}
$$
%\end{align*}

\end{document}


































